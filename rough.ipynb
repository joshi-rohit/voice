{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain deepgram-sdk langchain-groq langchain-openai torch transformers pillow python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show me how toload .env using os.environ\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain for Multimodal LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import langchain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import (\n",
    "     ChatPromptTemplate,\n",
    "     MessagesPlaceholder,\n",
    "     SystemMessagePromptTemplate,\n",
    "     HumanMessagePromptTemplate,\n",
    " )\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.tools import BaseTool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM : Groq-hosted Mixtral "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\", groq_api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Latency Large Language Models (LLMs) are a type of artificial intelligence model that can process and generate human-like text with minimal delay or latency. The importance of low latency in LLMs can be explained through the following points:\n",
      "\n",
      "1. Improved user experience: Low latency ensures that the model responds quickly to user inputs, providing a smooth and responsive user experience. This is particularly important in real-time applications such as chatbots, virtual assistants, and interactive games.\n",
      "2. Enhanced accuracy: LLMs with low latency are more likely to generate accurate and coherent responses, as they can process and consider the context of the user's input more effectively. High latency can result in the model missing important context or generating responses that are disconnected from the user's input.\n",
      "3. Better decision-making: In applications where LLMs are used for decision-making, such as financial trading or autonomous vehicles, low latency is critical for ensuring that the model can make timely and accurate decisions. Delays in processing information can result in missed opportunities or even dangerous situations.\n",
      "4. Scalability: Low latency LLMs are more scalable, as they can handle a higher volume of requests without becoming overwhelmed. This is important for applications that need to support a large number of users or requests.\n",
      "5. Competitive advantage: In some industries, low latency can provide a competitive advantage. For example, in online gaming or trading platforms, low latency can provide a faster and more responsive user experience, giving users an edge over their competitors.\n",
      "\n",
      "Overall, low latency is an important factor in LLMs, as it ensures that the model can process and respond to user inputs quickly and accurately, providing a better user experience, enhanced accuracy, improved decision-making, scalability, and competitive advantage.\n"
     ]
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "out = chain.invoke({\"text\":\"Explain the importance of low latency LLMs.\"})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's a bright ball of gas in the sky,\n",
      "That rises and sets, making spirits high.\n",
      "It gives us light and warmth,\n",
      "On sunny days it transforms,\n",
      "The world into a golden, cheery pi.\n",
      "\n",
      "(Note: I tried to make the last line \"The world into a golden, cheery high\", but that didn't fit the rhythm of a limerick. So I changed it to \"pi\", which is a mathematical constant and a playful way to end the limerick.)\n",
      "Silent, luminous,\n",
      "Glowing in the velvet night,\n",
      "The Moon's gentle light."
     ]
    }
   ],
   "source": [
    "# Test Async generation and streaming\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a Limerick about {topic}\")])\n",
    "chain = prompt | llm | output_parser\n",
    "out = await chain.ainvoke({\"topic\": \"The Sun\"})\n",
    "print(out)\n",
    "#await chain.ainvoke()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a haiku about {topic}\")])\n",
    "chain = prompt | llm\n",
    "for chunk in chain.stream({\"topic\": \"The Moon\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Model : Deepgram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech: deepgram\n",
    "from deepgram import (\n",
    "    DeepgramClient,\n",
    "    DeepgramClientOptions,\n",
    "    LiveTranscriptionEvents,\n",
    "    LiveOptions,\n",
    "    Microphone,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal: Image processing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# image processing\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model to be used\n",
    "hf_model = \"Salesforce/blip-image-captioning-large\"\n",
    "# use GPU if it's available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# preprocessor will prepare images for the model\n",
    "processor = BlipProcessor.from_pretrained(hf_model)\n",
    "# then we initialize the model itself\n",
    "model = BlipForConditionalGeneration.from_pretrained(hf_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a monkey that is sitting in a tree\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_url = 'https://images.unsplash.com/photo-1616128417859-3a984dd35f02?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2372&q=80' \n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "#image\n",
    "# unconditional image captioning\n",
    "inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(**inputs, max_new_tokens=30)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating it as a llm tool\n",
    "desc = (\n",
    "    \"use this tool when given the URL of an image that you'd like to be \"\n",
    "    \"described. It will return a simple caption describing the image.\"\n",
    "\n",
    ")\n",
    "\n",
    "class ImageCaptionTool(BaseTool):\n",
    "    name = \"Image captioner\"\n",
    "    description = desc\n",
    "    \n",
    "    def _run(self, url: str):\n",
    "        # download the image and convert to PIL object\n",
    "        image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "        # preprocess the image\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        # generate the caption\n",
    "        out = model.generate(**inputs, max_new_tokens=20)\n",
    "        # get the caption\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "tools = [ImageCaptionTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example agent for LLM\n",
    "from math import pi, sqrt, cos, sin\n",
    "from typing import Union, Optional\n",
    " \n",
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"Circumference calculator\"\n",
    "    description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        return float(radius)*2.0*pi\n",
    "\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "\n",
    "\n",
    "class PythagorasTool(BaseTool):\n",
    "    name = \"Hypotenuse calculator\"\n",
    "    description = (\n",
    "    \"\"\"use this tool when you need to calculate the length of a hypotenuse\n",
    "    using the given one or two sides of a triangle and/or an angle (in degrees). \n",
    "    To use the tool, you must provide at least two of the following parameters \n",
    "    ['adjacent_side', 'opposite_side', 'angle']. \n",
    "    If there are two lengths provided, then use the length of the first one as adjacent side and the second one as opposite side.\n",
    "    If there is only one length provided and there is an angle, then use the length as adjacent side and the angle as the angle.\n",
    "    if there is only one length provided and there is no angle, then tell the user to provide an angle or provide two lengths.\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    def _run(\n",
    "        self,\n",
    "        adjacent_side: Optional[Union[int, float]] = None,\n",
    "        opposite_side: Optional[Union[int, float]] = None,\n",
    "        angle: Optional[Union[int, float]] = None\n",
    "    ):\n",
    "        return f\"Adjacent side is {adjacent_side}, opposite side is {opposite_side}, and angle is {angle}.\"\n",
    "        # check for the values we have been given\n",
    "        if adjacent_side and opposite_side:\n",
    "            return sqrt(float(adjacent_side)**2 + float(opposite_side)**2)\n",
    "        elif adjacent_side and angle:\n",
    "            return adjacent_side / cos(float(angle))\n",
    "        elif opposite_side and angle:\n",
    "            return opposite_side / sin(float(angle))\n",
    "        else:\n",
    "            return \"Could not calculate the hypotenuse of the triangle. Need two or more of `adjacent_side`, `opposite_side`, or `angle`.\"\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "        memory_key='chat_history',\n",
    "        k=5,\n",
    "        return_messages=True\n",
    ")\n",
    "\n",
    "# Agent examples\n",
    "tools = [CircumferenceTool()]\n",
    "\n",
    "# initialize agent with tools\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")\n",
    "\n",
    "sys_msg = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Unfortunately, Assistant is terrible at maths. When provided with math questions, no matter how simple, assistant always refers to it's trusty tools and absolutely does NOT try to answer math questions by itself\n",
    "\n",
    "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "\"\"\"\n",
    "tools = [CircumferenceTool(), PythagorasTool()]\n",
    "\n",
    "new_prompt = agent.agent.create_prompt(\n",
    "    system_message=sys_msg,\n",
    "    tools=tools\n",
    ")\n",
    "agent.agent.llm_chain.prompt = new_prompt\n",
    "\n",
    "\n",
    "# update the agent tools\n",
    "agent.tools = tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"can you calculate the circumference of a circle that has a radius of 7.81mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"Tell me a poem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deepgram Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptCollector:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.transcript_parts = []\n",
    "\n",
    "    def add_part(self, part):\n",
    "        self.transcript_parts.append(part)\n",
    "\n",
    "    def get_full_transcript(self):\n",
    "        return ' '.join(self.transcript_parts)\n",
    "\n",
    "transcript_collector = TranscriptCollector()\n",
    "\n",
    "async def get_transcript(callback):\n",
    "    transcription_complete = asyncio.Event()  # Event to signal transcription completion\n",
    "\n",
    "    try:\n",
    "        # example of setting up a client config. logging values: WARNING, VERBOSE, DEBUG, SPAM\n",
    "        config = DeepgramClientOptions(options={\"keepalive\": \"true\"})\n",
    "        deepgram: DeepgramClient = DeepgramClient(\"\", config)\n",
    "\n",
    "        dg_connection = deepgram.listen.asynclive.v(\"1\")\n",
    "        print (\"Listening...\")\n",
    "\n",
    "        async def on_message(self, result, **kwargs):\n",
    "            sentence = result.channel.alternatives[0].transcript\n",
    "            \n",
    "            if not result.speech_final:\n",
    "                transcript_collector.add_part(sentence)\n",
    "            else:\n",
    "                # This is the final part of the current sentence\n",
    "                transcript_collector.add_part(sentence)\n",
    "                full_sentence = transcript_collector.get_full_transcript()\n",
    "                # Check if the full_sentence is not empty before printing\n",
    "                if len(full_sentence.strip()) > 0:\n",
    "                    full_sentence = full_sentence.strip()\n",
    "                    print(f\"Human: {full_sentence}\")\n",
    "                    callback(full_sentence)  # Call the callback with the full_sentence\n",
    "                    transcript_collector.reset()\n",
    "                    transcription_complete.set()  # Signal to stop transcription and exit\n",
    "\n",
    "        dg_connection.on(LiveTranscriptionEvents.Transcript, on_message)\n",
    "\n",
    "        options = LiveOptions(\n",
    "            model=\"nova-2\",\n",
    "            punctuate=True,\n",
    "            language=\"en-US\",\n",
    "            encoding=\"linear16\",\n",
    "            channels=1,\n",
    "            sample_rate=16000,\n",
    "            endpointing=300,\n",
    "            smart_format=True,\n",
    "        )\n",
    "\n",
    "        await dg_connection.start(options)\n",
    "\n",
    "        # Open a microphone stream on the default input device\n",
    "        microphone = Microphone(dg_connection.send)\n",
    "        microphone.start()\n",
    "\n",
    "        await transcription_complete.wait()  # Wait for the transcription to complete instead of looping indefinitely\n",
    "\n",
    "        # Wait for the microphone to close\n",
    "        microphone.finish()\n",
    "\n",
    "        # Indicate that we've finished\n",
    "        await dg_connection.finish()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not open socket: {e}\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:\n",
    "    def __init__(self):\n",
    "        self.transcription_response = \"\"\n",
    "        #self.llm = LanguageModelProcessor()\n",
    "\n",
    "    async def main(self):\n",
    "        def handle_full_sentence(full_sentence):\n",
    "            self.transcription_response = full_sentence\n",
    "\n",
    "        # Loop indefinitely until \"goodbye\" is detected\n",
    "        while True:\n",
    "            await get_transcript(handle_full_sentence)\n",
    "            \n",
    "            # Check for \"goodbye\" to exit the loop\n",
    "            if \"goodbye\" in self.transcription_response.lower():\n",
    "                break\n",
    "            \n",
    "            print(self.transcription_response)\n",
    "            # llm_response = self.llm.process(self.transcription_response)\n",
    "\n",
    "            # tts = TextToSpeech()\n",
    "            # tts.speak(llm_response)\n",
    "\n",
    "            # Reset transcription_response for the next loop iteration\n",
    "            self.transcription_response = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object ConversationManager.main at 0x146131560>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager = ConversationManager()\n",
    "manager.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
